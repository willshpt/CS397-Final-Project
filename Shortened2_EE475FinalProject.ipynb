{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uowmBgovw4OG",
        "NAIThcjH2f5b",
        "fiLC90P_2cQN",
        "0ve63ZhS75jV",
        "HR-ZZHka2sIv",
        "SY_qyjnCOugh",
        "J-cTnQDRHxyI",
        "GfxoEmMsJ-Mo",
        "OAUvcabWVz-h",
        "_Y96lrmLNC8D",
        "BzmOwLC1JzqV"
      ],
      "mount_file_id": "1zvfFozK-xQ082aZLFb5e37-uj7Bbver1",
      "authorship_tag": "ABX9TyPbBWukPD+iPoaUlaDCdRX0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/willshpt/CS397-Final-Project/blob/main/Shortened2_EE475FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eye Tracking with Webcams"
      ],
      "metadata": {
        "id": "KU9RiWUjwt_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Based Eye Tracking"
      ],
      "metadata": {
        "id": "2mFm06wKAwAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### My Method\n",
        "\n",
        "While not fully finished, here is my basic work on designing my own ML-based eye tracking."
      ],
      "metadata": {
        "id": "CHropNvyP-0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Research"
      ],
      "metadata": {
        "id": "ukERoE1LS4OK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before doing any research into the project, I came up with a basic idea of how my implementation would work. I had a feeling that I wouldn't be able to do face/eye detection from scratch using only the tools we had from class with the time I had. Instead, my plan was to use an existing face/eye/pupil detection model to generate an input for my eye tracking model.\n",
        "\n",
        "For data, I would create my own dataset by creating a program which would display a pixel on a screen and then take a webcam image of myself (or anybody I could get to participate) once a button is pressed confirming that I am looking at the pixel. That image and the pixel's x and y coordinates would be stored.\n",
        "\n",
        "To train my model, I would run each captured image through the existing face/eye/pupil detection model, which would give features which my own model could use as inputs, with the x and y coordinates of the pixel as outputs. I would then train my model using this data, testing out different models and different hyperparameters to find which one would be most consistent.\n",
        "\n",
        "With the time and resources I had, I was unable to create a dataset of my own in time to get a working model. However, I have supplemental research on similar projects to discuss their pros and cons, and how I might modify my project in order to make it better."
      ],
      "metadata": {
        "id": "_m4CLpHbS-ZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### GazeCapture"
      ],
      "metadata": {
        "id": "OAUvcabWVz-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing I found when looking for datasets similar to what I was planning on doing was [GazeCapture](https://github.com/CSAILVision/GazeCapture.git) \\(their paper can be found [here](https://gazecapture.csail.mit.edu/index.php)). This is one of the few projects I was able to find where they were doing almost exactly what I was looking for -- not just detecting the direction of a person's gaze, but rather looking at where on a screen a person was looking. They created the GazeCapture dataset by uploading an application to the iOS app store which would display pulsating red dots on screen at various positions and record the user's face cam as they look at the dots. There are other parts of the application, such as making the user press left or right on the screen depending on if there is an L or an R displayed in the dot, in order to make sure the user is engaged, but otherwise the idea is almost exactly what I was thinking of in order to collect data."
      ],
      "metadata": {
        "id": "MCT_aVTMWARD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this data, they trained a Convolutional Neural Network, which they named iTracker, for gaze prediction. iTracker did not use any pre-existing face or eye detection, and was able to outperform similar approaches by a significant margin, according to the paper. However, the iTracker model was too complex to run in real-time due to the size of the inputs."
      ],
      "metadata": {
        "id": "WNwqilwibgTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Convolutional Neural Network-Based Methods for Eye Gaze Estimation: A Survey"
      ],
      "metadata": {
        "id": "_Y96lrmLNC8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the other sources I found which discussed eye tracking in a more broad sense is [Convolutional Neural Network-Based Methods for Eye Gaze Estimation: A Survey](https://ieeexplore.ieee.org/abstract/document/9153754) which is a survey of different deep learning-based gaze estimation techniques, focusing on convolutional nerual networks, as well as other methods. The paper discusses 2D and 3D approaches to eye tracking, and was focused more on explaining the general types of eye tracking."
      ],
      "metadata": {
        "id": "02z6HvG0NEyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Actual Code"
      ],
      "metadata": {
        "id": "HWz85LgVis8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### A model to detect looking left and right on the screen"
      ],
      "metadata": {
        "id": "GGRm_dJLEJCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea here is to create a dataset by grabbing images of the user looking left and right, then to use neural network boosting to train a 2 class dataset with training and validation. At the end, I have a way of testing the model in real time. This is very finnicky, but I was able to get decent results even with only 20 datapoints, especially considering how little I had tuned the pupil threshold and the hyperparameters of the model.\n",
        "\n",
        "Of course, this is still pretty much the same as the first example model I provided, and it is a multiclass model when ideally it should be either one multi-output regression model or two single-output regression models, however I plan on improving this in the future."
      ],
      "metadata": {
        "id": "uQi1y7zvewhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For length reasons, the method used for collecting the data is not shown here, but the code for training the model is shown."
      ],
      "metadata": {
        "id": "BeUCKAyiTveb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1, y1 = collect_data(num_its_per_direction)\n",
        "print(x1)\n",
        "print(y1)"
      ],
      "metadata": {
        "id": "ndQi-CjKT-bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import basic librariees and autograd wrapped numpy\n",
        "from sklearn.datasets import fetch_openml\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "import autograd.numpy as np2\n",
        "import matplotlib.pyplot as plt\n",
        "from autograd import grad\n",
        "from autograd import hessian\n",
        "from autograd import value_and_grad\n",
        "from autograd.misc.flatten import flatten_func\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# this is needed to compensate for matplotlib notebook's tendancy to blow up images when plotted inline\n",
        "%matplotlib inline\n",
        "#from matplotlib import rcParams\n",
        "#rcParams['figure.autolayout'] = True\n",
        "\n",
        "# datapath to data\n",
        "datapath = '/content/drive/MyDrive/Colab Notebooks/mlrefined_datasets/nonlinear_superlearn_datasets/'\n",
        "\n",
        "\n",
        "from numba import njit, prange"
      ],
      "metadata": {
        "id": "KlxCVmZyUFkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x4 = np2.array(x1).T\n",
        "y4new = np2.array(y1)[np2.newaxis,:]\n",
        "print(np2.shape(x4))\n",
        "print(np2.shape(y4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXNj0YcNVhEb",
        "outputId": "b096a38b-94bd-41d6-fc8d-c9affdc2700c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 40)\n",
            "(1, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# newtons method function - inputs: g (input function), max_its (maximum number of iterations), w (initialization)\n",
        "def newtons_method(g,max_its,w,**kwargs):\n",
        "    # flatten input funciton, in case it takes in matrices of weights\n",
        "    flat_g, unflatten, w = flatten_func(g, w)\n",
        "\n",
        "    # compute the gradient / hessian functions of our input function -\n",
        "    # note these are themselves functions.  In particular the gradient -\n",
        "    # - when evaluated - returns both the gradient and function evaluations (remember\n",
        "    # as discussed in Chapter 3 we always ge the function evaluation 'for free' when we use\n",
        "    # an Automatic Differntiator to evaluate the gradient)\n",
        "    gradient = value_and_grad(flat_g)\n",
        "    hess = hessian(flat_g)\n",
        "     # set numericxal stability parameter / regularization parameter\n",
        "    epsilon = 10**(-7)\n",
        "    if 'epsilon' in kwargs:\n",
        "        epsilon = kwargs['epsilon']\n",
        "\n",
        "    # run the newtons method loop\n",
        "    weight_history = []      # container for weight history\n",
        "    cost_history = []        # container for corresponding cost function history\n",
        "    for k in range(max_its):\n",
        "        # evaluate the gradient, store current weights and cost function value\n",
        "        cost_eval,grad_eval = gradient(w)\n",
        "        weight_history.append(unflatten(w))\n",
        "        cost_history.append(cost_eval)\n",
        "\n",
        "        # evaluate the hessian\n",
        "        hess_eval = hess(w)\n",
        "\n",
        "        # reshape for numpy linalg functionality\n",
        "        hess_eval.shape = (int((np2.size(hess_eval))**(0.5)),int((np2.size(hess_eval))**(0.5)))\n",
        "\n",
        "        # solve second order system system for weight update\n",
        "        w = w - np2.dot(np2.linalg.pinv(hess_eval + epsilon*np2.eye(np.size(w))),grad_eval)\n",
        "\n",
        "    # collect final weights\n",
        "    weight_history.append(unflatten(w))\n",
        "    # compute final cost function value via g itself (since we aren't computing\n",
        "    # the gradient at the final step we don't get the final cost function value\n",
        "    # via the Automatic Differentiatoor)\n",
        "    cost_history.append(flat_g(w))\n",
        "    return cost_history,weight_history"
      ],
      "metadata": {
        "id": "bOKfssyvZcOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_indices4 = np2.sort(np2.random.choice(np2.shape(x4)[1], int(2*np2.shape(x4)[1]/3), replace=False))\n",
        "validation_indices4 = np2.delete(np2.arange(np.shape(x4)[1]), training_indices4)\n",
        "x4train = x4.T[training_indices4].T\n",
        "x4valid = x4.T[validation_indices4].T\n",
        "y4train = y4[0][training_indices4]\n",
        "y4valid = y4[0][validation_indices4]\n",
        "print(x4.shape)\n",
        "print(x4train.shape)\n",
        "print(x4valid.shape)"
      ],
      "metadata": {
        "id": "GuNHWYqiZjxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initial_model4(x,w):\n",
        "  return (w*np2.ones((1,x.shape[1])))"
      ],
      "metadata": {
        "id": "VwuaNUQuZ4Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax4_train(w,model):\n",
        "  cost = np2.sum(np2.log(1 + np2.exp(-y4train*model(x4train,w))))\n",
        "  a = cost/float(np2.size(y4train))\n",
        "  return a"
      ],
      "metadata": {
        "id": "OmI9r4-fZ7Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax4_validate(w,model):\n",
        "  cost = np2.sum(np2.log(1 + np2.exp(-y4valid*model(x4valid,w))))\n",
        "  a = cost/float(np2.size(y4valid))\n",
        "  return a"
      ],
      "metadata": {
        "id": "yPuhlnUmZ8pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax4_overall(w,model):\n",
        "  cost = np2.sum(np2.log(1 + np2.exp(-y4*model(x4,w))))\n",
        "  a = cost/float(np2.size(y4))\n",
        "  return a"
      ],
      "metadata": {
        "id": "xVPIYkj3Z93Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_transforms4(x,w):\n",
        "    # No transform\n",
        "    f = x\n",
        "    return f"
      ],
      "metadata": {
        "id": "v6siMFmDaCil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron4(x, w):\n",
        "        # compute inner product with current layer weights\n",
        "        f = feature_transforms4(x,w)\n",
        "        a = w[0][0] + np2.dot(f.T, w[0][1:])\n",
        "        # output of layer activation\n",
        "        a = np2.maximum(0,a).T\n",
        "        # final linear combo\n",
        "        a = w[1][0] + np2.dot(a.T,w[1][1:])\n",
        "        return a.T"
      ],
      "metadata": {
        "id": "mQcH_SBDZ_b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def misclass_calc(model, x, true_labels, threshold=0):\n",
        "    # Get model predictions\n",
        "    predicted_probs = model(x)\n",
        "    # Convert predicted probabilities to binary predictions based on the threshold\n",
        "    binary_predictions = (predicted_probs >= threshold).astype(int)\n",
        "    binary_predictions = np2.array([-1 if x == 0 else x for x in binary_predictions[0]])\n",
        "    # Count misclassifications\n",
        "    misclassifications = np2.sum(binary_predictions != true_labels)\n",
        "    return misclassifications\n"
      ],
      "metadata": {
        "id": "H_EwNx-oa06A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_net_boosting_learner(num_steps, max_its, scale):\n",
        "  step_array = np.arange(1,num_steps)\n",
        "  best_units = []\n",
        "  model_history = []\n",
        "  training_cost_history = []\n",
        "  validation_cost_history = []\n",
        "  combined_cost_history = []\n",
        "  training_misclassification_history = []\n",
        "  validation_misclassification_history = []\n",
        "  combined_misclassification_history = []\n",
        "  w_best_history = []\n",
        "  model_0 = initial_model4;\n",
        "  unfinished = True;\n",
        "  s_factor = 1.0\n",
        "  while unfinished:\n",
        "    scale_temp = round(scale*s_factor,4);\n",
        "    print(\"Step: \", 0)\n",
        "    print(\"Scale: \", scale_temp)\n",
        "    w = scale_temp*np2.random.randn(1)\n",
        "    try:\n",
        "      cost_hist, weight_hist = newtons_method(lambda w : softmax4_train(w, model_0), max_its, w)\n",
        "      unfinished = False\n",
        "    except np2.linalg.LinAlgError:\n",
        "      print(\"Did not converge! Decreasing scale\")\n",
        "      s_factor = s_factor - 0.1\n",
        "  ind = np2.argmin(cost_hist)\n",
        "  w_best = weight_hist[ind]\n",
        "  w_best_history.append(w_best)\n",
        "  model = lambda x,w=w_best : model_0(x,w)\n",
        "  best_units.append(model)\n",
        "  model_history.append(model)\n",
        "  training_cost_history.append(softmax4_train(w_best,model))\n",
        "  validation_cost_history.append(softmax4_validate(w_best,model))\n",
        "  combined_cost_history.append(softmax4_overall(w_best,model))\n",
        "  training_misclassification_history.append(misclass_calc(model, x4train, y4train))\n",
        "  validation_misclassification_history.append(misclass_calc(model, x4valid, y4valid))\n",
        "  combined_misclassification_history.append(misclass_calc(model, x4, y4))\n",
        "  for j in step_array:\n",
        "    next_unit = lambda x,w: perceptron4(x,w)\n",
        "    current_model = lambda x,w: model(x) + next_unit(x,w)\n",
        "    unfinished = True;\n",
        "    s_factor = 1.0\n",
        "    while unfinished:\n",
        "      scale_temp = round(scale*s_factor,4);\n",
        "      print(\"Step: \", j)\n",
        "      print(\"Scale: \", scale_temp)\n",
        "      w = [scale_temp*np2.random.randn(x4train.shape[0] + 1,1), scale_temp*np2.random.randn(2,1), scale_temp*np.random.randn(6,1)]\n",
        "      try:\n",
        "        cost_hist, weight_hist = newtons_method(lambda w : softmax4_train(w, current_model), max_its, w)\n",
        "        unfinished = False\n",
        "      except np2.linalg.LinAlgError:\n",
        "        print(\"Did not converge! Decreasing scale\")\n",
        "        s_factor = s_factor * 0.9\n",
        "    ind = np2.argmin(cost_hist)\n",
        "    w_best = weight_hist[ind]\n",
        "    w_best_history.append(w_best)\n",
        "    best_perceptron = lambda x,w=w_best: next_unit(x,w)\n",
        "    best_units.append(best_perceptron)\n",
        "    next_model = lambda x,w=w_best : current_model(x,w)\n",
        "    model_history.append(next_model)\n",
        "    training_cost_history.append(softmax4_train(w_best,next_model))\n",
        "    validation_cost_history.append(softmax4_validate(w_best,next_model))\n",
        "    combined_cost_history.append(softmax4_overall(w_best,next_model))\n",
        "    training_misclassification_history.append(misclass_calc(next_model, x4train, y4train))\n",
        "    validation_misclassification_history.append(misclass_calc(next_model, x4valid, y4valid))\n",
        "    combined_misclassification_history.append(misclass_calc(next_model, x4, y4))\n",
        "    model = lambda x,units=best_units: np2.sum([v(x) for v in units],axis=0)\n",
        "    print(\"Misclassifications: \", combined_misclassification_history[-1])\n",
        "  return w_best_history, best_units, model_history, training_cost_history, validation_cost_history, combined_cost_history, training_misclassification_history, validation_misclassification_history, combined_misclassification_history"
      ],
      "metadata": {
        "id": "Y4zvyOz0aUzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps4 = 20\n",
        "max_its4 = 60\n",
        "scale4 = .5\n",
        "w_best_hist4, best_units4, model_history4, tcost_hist4, vcost_hist4, ccost_hist4, tmisclass_hist4, vmisclass_hist4, cmisclass_hist4 = neural_net_boosting_learner(num_steps4,max_its4,scale4)\n",
        "true_steps = np.arange(0, np.array(tcost_hist4).shape[0])\n",
        "plt.plot(true_steps, tcost_hist4, label=\"Training Cost\")\n",
        "plt.plot(true_steps, vcost_hist4, label=\"Validation Cost\")\n",
        "plt.plot(true_steps, ccost_hist4, label=\"Overall Cost\")\n",
        "plt.xlabel('Num Steps')\n",
        "plt.ylabel('Cost')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.plot(true_steps, tmisclass_hist4, label=\"Training Misclassifications\")\n",
        "plt.plot(true_steps, vmisclass_hist4, label=\"Validation Misclassifications\")\n",
        "plt.plot(true_steps, cmisclass_hist4, label=\"Overall Misclassifications\")\n",
        "plt.xlabel('Num Steps')\n",
        "plt.ylabel('Misclassifications')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N-jMxGwwaZIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_ind4 = np.argmin(cmisclass_hist4)\n",
        "best_model4 = model_history4[best_ind4]\n",
        "w4f = w_best_hist4[best_ind4]\n",
        "print(\"Misclassifications: \", misclass_calc(best_model4,x4,y4))\n",
        "#print(\"Predictions: \", best_model4(x4,))"
      ],
      "metadata": {
        "id": "tJDdkSrRb6PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analysis and Future Work"
      ],
      "metadata": {
        "id": "RO7hvtrl3XG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of my simple left-right gaze detector using neural network boosting with softmax ended up being much more successful than I thought it would be going in.\n",
        "\n",
        "On my first attempt, with a sample size of 20 admittedly rough input datapoints, I was able to get zero misclassifications on the training and validation sets. Given a bit more time and a local Python environment running on a decent computer and a larger, more robust dataset, I believe it would be more than possible to take this project and iterate upon it to get a decently reliable gaze tracker capable of estimating where on a screen the user is looking."
      ],
      "metadata": {
        "id": "iu2vdmEF3gNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sources"
      ],
      "metadata": {
        "id": "BzmOwLC1JzqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antoinelame Gaze Tracking: https://github.com/antoinelame/GazeTracking\n",
        "\n",
        "Convolutional Neural Network-Based Methods for Eye Gaze Estimation: A Survey: https://ieeexplore.ieee.org/abstract/document/9153754\n",
        "\n",
        "DougDoug: https://www.youtube.com/watch?v=6cI_D2dfw24\n",
        "\n",
        "Eye Tracking for Everyone: https://gazecapture.csail.mit.edu/index.php\n",
        "\n",
        "Tracking your eyes with Python: https://medium.com/@stepanfilonov/tracking-your-eyes-with-python-3952e66194a6\n",
        "\n",
        "Wikipedia: https://en.wikipedia.org/wiki/Eye_tracking"
      ],
      "metadata": {
        "id": "jytXT5UxJ2SD"
      }
    }
  ]
}